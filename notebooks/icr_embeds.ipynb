{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGA0r368ZzoM"
   },
   "source": [
    "# Word Embeddings for World Bank ICR Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "up2kx2Ep3qfq",
    "outputId": "eb3f8f44-6d16-4552-9088-731d99fda840"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score, cross_validate\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from utils import FILES, FILE2ID, FILE2SECTOR, read_file\n",
    "nltk.download('punkt')\n",
    "# nlp = spacy.load(\"en_core_web_sm\",  disable=[\"tagger\", \"ner\", \"parser\"])\n",
    "# nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ixoMgVkHA10n",
    "outputId": "424ac687-b10c-4b77-e9ad-6342eb06cefe"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5yHPxBb4rVI"
   },
   "outputs": [],
   "source": [
    "# copy reports\n",
    "!cp /content/drive/MyDrive/WorldBank/* .\n",
    "\n",
    "# unzip reports\n",
    "!unzip -q icr_text_docs.zip -d icr\n",
    "!mv icr/documents/* icr/ && rm -rf icr/documents *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96uQYwRHalV0",
    "outputId": "e27d4b14-144f-4440-d400-f911fa7b7752"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5l_pEad142_q"
   },
   "outputs": [],
   "source": [
    "# Create lookup dicts\n",
    "FILES = glob.glob(\"icr/P0*_0*-*\")\n",
    "\n",
    "def file2id(filename):\n",
    "    assert 'icr/' in filename\n",
    "    return filename[4:11]\n",
    "\n",
    "FILE2ID = {file: file2id(file) for file in FILES}\n",
    "\n",
    "# Get sectors lookup and make dataframe\n",
    "sector_df = pd.read_csv('clean_dli_pdo_embeds_sector.csv')\n",
    "sector_df.parent_sector_name = sector_df.parent_sector_name.fillna('None') # replace nan\n",
    "\n",
    "ID2SECTOR = {}\n",
    "for projectid, sector_name in sector_df[['id','parent_sector_name']].values:\n",
    "    ID2SECTOR[projectid] = sector_name    \n",
    "\n",
    "FILE2SECTOR = {file: ID2SECTOR[FILE2ID[file]] for file in FILES}\n",
    "\n",
    "def file2words(file):\n",
    "    \"\"\"Extract words as tokens from file with nltk, lemmatize, remove stop words and filter\"\"\"\n",
    "    for encoding in ['utf-8', 'iso-8859-15']:\n",
    "        try:\n",
    "            with open(file, 'r', encoding=encoding) as f:\n",
    "                text = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "            \n",
    "    valid = [w.lower() for w in nltk.wordpunct_tokenize(text) if w.lower() in WORDS and len(w) > 2]\n",
    "\n",
    "    # lemmatize\n",
    "    valid = [lemmatizer.lemmatize(w) for w in valid]\n",
    "\n",
    "    # remove stop words\n",
    "    valid = [w for w in valid if w not in STOPWORDS]\n",
    "    return valid\n",
    "\n",
    "def get_most_common(words):\n",
    "    \"\"\"Get most common words\"\"\"\n",
    "    fdist1 = nltk.FreqDist(words)\n",
    "    filtered_word_freq = dict((word, freq) for word, freq in fdist1.items() if not word.isdigit())\n",
    "    c = Counter(filtered_word_freq)\n",
    "    return c.most_common()\n",
    "\n",
    "    \n",
    "def read_file(file):\n",
    "    for encoding in ['utf-8', 'iso-8859-15']:\n",
    "        try:\n",
    "            with open(file, 'r', encoding=encoding) as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise UnicodeDecodeError\n",
    "\n",
    "def project(embeddings, dims=2):\n",
    "    X = embeddings\n",
    "    pca = PCA(n_components=dims)\n",
    "    projections = pca.fit_transform(X)\n",
    "    if dims == 2:\n",
    "        PC1, PC2 = np.hsplit(projections, 2)\n",
    "        return {'PC1': PC1.flatten(), 'PC2': PC2.flatten()}\n",
    "    elif dims == 3:\n",
    "        PC1, PC2, PC3 = np.hsplit(projections, 3)\n",
    "        return {'PC1': PC1.flatten(), 'PC2': PC2.flatten(), 'PC3': PC3.flatten()}\n",
    "\n",
    "def clean_sentences(text):\n",
    "  sentences = nltk.tokenize.sent_tokenize(text)\n",
    "\n",
    "  # Clean up sentences from puctuation\n",
    "  cleaned = []\n",
    "  for sentence in sentences:\n",
    "      # split into words\n",
    "      tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "      # remove all tokens that are not alphabetic\n",
    "      clean_sentence = \" \".join(word for word in tokens if word.isalpha() and len(word) > 2)\n",
    "      cleaned.append(clean_sentence)\n",
    "  return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzKlZR0XCqYH"
   },
   "source": [
    "## TF-IFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-XqQdWPyCsSy",
    "outputId": "fb8fb8c9-c027-4b9e-a352-630e293f7080"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "dfs = []\n",
    "for file in FILES:\n",
    "  text = read_file(file)\n",
    "  dfs.append({'text': text, 'file': file})\n",
    "df = pd.DataFrame(dfs)\n",
    "df['sector'] = df.file.apply(lambda x: FILE2SECTOR[x])\n",
    "df.sector = df.sector.astype('category')\n",
    "\n",
    "# clean sectors column\n",
    "sectors = [x for x in df.sector.unique() if not x.startswith('(H)')]\n",
    "df = df[df.sector.isin(sectors)]\n",
    "\n",
    "# get PCs\n",
    "pcs = pd.DataFrame({'sector':df.sector, 'project': df.file.apply(lambda x: FILE2ID[x]), **project(X, dims=3)})\n",
    "\n",
    "## 1) Lower\n",
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "## 2) Remove tags\n",
    "df[\"text\"] = df.apply(lambda x: re.sub(\"<[^>]*>\", \"\", x[\"text\"]), axis=1)\n",
    "## 3) Tokenize\n",
    "df[\"text_proc\"] = df.apply(lambda x: word_tokenize(x[\"text\"]), axis=1)\n",
    "## 4) Remove punctuation\n",
    "df[\"text_proc\"] = df.apply(lambda x: [w.translate(table) for w in x[\"text_proc\"]], axis=1)\n",
    "## 5) Remove non-alpha\n",
    "df[\"text_proc\"] = df.apply(lambda x: [w for w in x[\"text_proc\"] if w.isalpha()], axis=1)\n",
    "## 6) Remove stop-words  \n",
    "\n",
    "df[\"text_proc\"] = df.apply(lambda x: [w for w in x[\"text_proc\"] if not w in stop_words], axis=1)\n",
    "## 7) Reformat to have a single text. \n",
    "df[\"text_proc_res\"] = df.apply(lambda x: ' '.join(x[\"text_proc\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gi-9e7f9E1wU",
    "outputId": "b2cbf74a-b906-4a61-c949-79d739c8732a"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "vec = TfidfVectorizer(stop_words='english', max_df = 0.95, min_df=2, max_features=1000)\n",
    "x = vec.fit_transform(df[\"text_proc_res\"])\n",
    "\n",
    "print(x.shape)\n",
    "# reduce dimensionality\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "res = svd.fit_transform(x)\n",
    "\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmA69mANFNpd",
    "outputId": "656ef888-839b-4980-e36b-56b77aa95dbc"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "df.sector = df.sector.astype('category')\n",
    "y = df[\"sector\"].values\n",
    "X = res\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "model = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo')\n",
    "\n",
    "metrics = cross_validate(model, X, y, scoring=['precision_macro', 'recall_macro'], cv=cv, n_jobs=-1)\n",
    "\n",
    "print('Precision: %.3f (%.3f)' % (mean(metrics[\"test_precision_macro\"]), std(metrics[\"test_precision_macro\"])))\n",
    "print('Recall: %.3f (%.3f)' % (mean(metrics[\"test_recall_macro\"]), -std(metrics[\"test_recall_macro\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MURiGqsXUdGY",
    "outputId": "01057a75-9a19-4948-ce5a-b5bd5ed86c3c"
   },
   "outputs": [],
   "source": [
    "fig = plotly.subplots.make_subplots(rows=1, cols=1)\n",
    "\n",
    "sectors = sorted([x for x in pcs.sector.unique() if not x.startswith('(H)')])\n",
    "focus_sectors = ['Education', 'Health' ,'Water/Sanit/Waste', ]\n",
    "\n",
    "for sector in sectors:\n",
    "    sector_df = pcs[pcs['sector'] == sector]\n",
    "    if not len(sector_df.values):\n",
    "        print(f\"Skipping {sector}, no matches found\")\n",
    "        continue        \n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(mode='markers',\n",
    "                   x=sector_df.PC1, y=sector_df.PC2, \n",
    "                   z=sector_df.PC3,\n",
    "                   text=sector_df.project,\n",
    "                   marker=dict(\n",
    "                           size=10,\n",
    "                   ),\n",
    "  \n",
    "                   name = sector_df.sector.values[0],\n",
    "                   hovertemplate = '%{text}',\n",
    "                    )\n",
    "        )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    # xaxis_title=\"PC1\",\n",
    "    # yaxis_title=\"PC2\",\n",
    "    # zaxis_title=\"PC3\",\n",
    "    title_text=f'World Bank ICR Reviews Term Frequency-Inverse Document Frequency'\n",
    ")\n",
    "fig.update_traces(textposition='top center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "lPYZER1cID0N",
    "outputId": "75c080cf-8a38-49de-ef8e-f94d996f12c6"
   },
   "outputs": [],
   "source": [
    "fig = plotly.subplots.make_subplots(rows=1, cols=1)\n",
    "\n",
    "sectors = sorted([x for x in pcs.sector.unique() if not x.startswith('(H)')])\n",
    "focus_sectors = ['Education', 'Health' ,'Water/Sanit/Waste', ]\n",
    "\n",
    "for sector in focus_sectors:\n",
    "    sector_df = pcs[pcs['sector'] == sector]\n",
    "    if not len(sector_df.values):\n",
    "        print(f\"Skipping {sector}, no matches found\")\n",
    "        continue        \n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(mode='markers',\n",
    "                   x=sector_df.PC1, y=sector_df.PC2, \n",
    "                  #  z=sector_df.PC3,\n",
    "                   text=sector_df.project,\n",
    "                   marker=dict(\n",
    "                           size=10,\n",
    "                   ),\n",
    "  \n",
    "                   name = sector_df.sector.values[0],\n",
    "                   hovertemplate = '%{text}',\n",
    "                    )\n",
    "        )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    # xaxis_title=\"PC1\",\n",
    "    # yaxis_title=\"PC2\",\n",
    "    # zaxis_title=\"PC3\",\n",
    "    title_text=f'World Bank ICR Reviews\\nTerm Frequency-Inverse Document Frequency Embeddings'\n",
    ")\n",
    "fig.update_traces(textposition='top center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cubXESvsCHrA"
   },
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MEO3xgEpONpb",
    "outputId": "4b3c59c2-e079-4132-f31e-31acda2fa7e9"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data, _ = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'),\n",
    "                             return_X_y=True)\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "n_samples = len(df)\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "t0 = time()\n",
    "data = df.text_proc_res.values\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "plot_top_words(nmf, tfidf_feature_names, n_top_words,\n",
    "               'Topics in NMF model (Frobenius norm)')\n",
    "\n",
    "# Fit the NMF model\n",
    "print('\\n' * 2, \"Fitting the NMF model (generalized Kullback-Leibler \"\n",
    "      \"divergence) with tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "plot_top_words(nmf, tfidf_feature_names, n_top_words,\n",
    "               'Topics in NMF model (generalized Kullback-Leibler divergence)')\n",
    "\n",
    "print('\\n' * 2, \"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, 'Topics in LDA model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EGeR3ApCWNLh",
    "outputId": "f5a9fb60-3596-4c6a-d274-29beb855ecd6"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "n_features = 1000\n",
    "n_components = 4\n",
    "n_top_words = 20\n",
    "n_samples = len(df)\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "t0 = time()\n",
    "data = df.text_proc_res.values\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "plot_top_words(nmf, tfidf_feature_names, n_top_words,\n",
    "               'Topics in NMF model (Frobenius norm)')\n",
    "\n",
    "# Fit the NMF model\n",
    "print('\\n' * 2, \"Fitting the NMF model (generalized Kullback-Leibler \"\n",
    "      \"divergence) with tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "plot_top_words(nmf, tfidf_feature_names, n_top_words,\n",
    "               'Topics in NMF model (generalized Kullback-Leibler divergence)')\n",
    "\n",
    "print('\\n' * 2, \"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, 'Topics in LDA model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUkzFAJpW_NN"
   },
   "source": [
    "## Siamese Sentence Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVO_4AcA3lVl"
   },
   "outputs": [],
   "source": [
    "EMBS_PATH = 'siamese_bert_report_albert_sent+embs.pk'\n",
    "icr_sentences = {}\n",
    "if not os.path.exists(EMBS_PATH):  \n",
    "    print(\"Generating embeddings\")\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-albert-small-v2')\n",
    "    report_embs = []\n",
    "    for file in tqdm(FILES):\n",
    "        text = read_file(file)   \n",
    "        # document = Doc(text)\n",
    "        cleaned_sentences = [x for x in clean_sentences(text) if len(x)]\n",
    "        icr_sentences[file] = cleaned_sentences\n",
    "        # sentences = [x.sent.text for x in a if len(x) > 15] # remove stubs\n",
    "        #Sentences are encoded by calling model.encode()\n",
    "        embeddings = model.encode([x for x in cleaned_sentences], batch_size=128)\n",
    "    #     PCs = project(embeddings, dims=3)\n",
    "    #     file_vecs = pd.DataFrame({'sentence': sentences, 'file': file, 'embedding': embeddings})\n",
    "        data = [{'file': file, 'embedding': embeddings[idx], 'sentence': sent} for idx, sent in enumerate(cleaned_sentences)]    \n",
    "        report_embs.extend(data)\n",
    "    pickle.dump(report_embs, open(EMBS_PATH, 'wb'))\n",
    "    !cp $EMBS_PATH /content/drive/MyDrive/WorldBank/$EMBS_PATH\n",
    "else:\n",
    "    report_embs = pickle.load(open(EMBS_PATH, 'rb'))\n",
    "\n",
    "# df = pd.concat(sent_vecs)\n",
    "# df.file = df.file.astype('category')\n",
    "# df.to_csv('siamese_bert_sent_vecs_pca.csv')\n",
    "# df = pd.read_csv('sent_vecs_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tr17IISASkrd"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(report_embs)\n",
    "df['sector'] = df.file.apply(lambda x: FILE2SECTOR[x])\n",
    "\n",
    "df.file = df.file.astype('category')\n",
    "df.sector = df.sector.astype('category')\n",
    "\n",
    "all_embeddings = np.vstack(df.embedding.values)\n",
    "df = pd.concat([df, pd.DataFrame(project(all_embeddings, dims=3))], axis=1)\n",
    "df['project'] = df.file.apply(lambda x: FILE2ID[x])\n",
    "\n",
    "drop_sectors = [s for s in df.sector.unique() if s.startswith('(H)')]\n",
    "df.drop(df[df.sector.isin(drop_sectors)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmbJZPFgcG9E"
   },
   "source": [
    "### Report Sentence Embeddings\n",
    "One data point per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "id": "kCIyHb8mr7Yi",
    "outputId": "683199d3-92cb-44a3-959b-a518f8a629a9"
   },
   "outputs": [],
   "source": [
    "sectors = ['Education', 'Health' ,'Water/Sanit/Waste', ]\n",
    "# sectors = [x for x in df.sector.unique() if not x.startswith('(H)')]\n",
    "fig = plotly.subplots.make_subplots(rows=1, cols=1)\n",
    "\n",
    "for sector in tqdm(sorted(sectors)):\n",
    "    sector_df = df[df['sector'] == sector]\n",
    "    if not len(sector_df.values):\n",
    "        print(f\"Skipping {sector}, no matches found\")\n",
    "        continue        \n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(mode='markers',\n",
    "                   x=sector_df.PC1, y=sector_df.PC2, \n",
    "                   z=sector_df.PC3,\n",
    "                   text=sector_df.sentence,\n",
    "                   marker=dict(\n",
    "#                            opacity=0.5,\n",
    "    #                          color=2,\n",
    "                           size=10,\n",
    "    #                         colorscale='Viridis',\n",
    "    #                         line_width=1\n",
    "                   ),\n",
    "    #                    customdata = np.dstack((sector_df.sector.values, sector_df.report_id.values)),\n",
    "                   name = sector_df.sector.values[0],\n",
    "                   hovertemplate = '%{text}',\n",
    "    #                      <br>Report: %{customdata[1]}',\n",
    "#                          fill=\"toself\",\n",
    "    #                visible='legendonly'\n",
    "                    )\n",
    "        )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    # xaxis_title=\"PC1\",\n",
    "    # yaxis_title=\"PC2\",\n",
    "    # zaxis_title=\"PC3\",\n",
    "    title_text=f'World Bank ICR Reviews'\n",
    ")\n",
    "fig.update_traces(textposition='top center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8Kkwm28cWcc"
   },
   "source": [
    "### Report Mean Embeddings\n",
    "One embedding per report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUomDuzoRYro"
   },
   "outputs": [],
   "source": [
    "report_mean_embeddings = []\n",
    "for file, group in df.groupby('file'):\n",
    "    if group.empty:\n",
    "        continue\n",
    "    mean_embedding = group.embedding.values.mean(0)\n",
    "    report_mean_embeddings.append({'file': file, 'mean_embedding': mean_embedding, 'sector': group.sector.values[0], 'project': group.project.values[0]})\n",
    "\n",
    "df = pd.DataFrame(report_mean_embeddings)\n",
    "\n",
    "all_embeddings = np.vstack(df.mean_embedding.values)\n",
    "df = pd.concat([df, pd.DataFrame(project(all_embeddings, dims=3))], axis=1)\n",
    "\n",
    "drop_sectors = [s for s in df.sector.unique() if s.startswith('(H)')]\n",
    "df.drop(df[df.sector.isin(drop_sectors)].index, inplace=True)\n",
    "\n",
    "df.file = df.file.astype('category')\n",
    "df.sector = df.sector.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFsW2f_Yc_78"
   },
   "source": [
    "### Focus sectors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "CLTGZMx5NwmH",
    "outputId": "65f206f5-ce61-46e2-911c-3ba1119a6131"
   },
   "outputs": [],
   "source": [
    "sectors = ['Education', 'Health', 'Water/Sanit/Waste']\n",
    "# sectors = [x for x in df.sector.unique() if not x.startswith('(H)')]\n",
    "fig = plotly.subplots.make_subplots(rows=1, cols=1)\n",
    "\n",
    "for sector in sorted(sectors):\n",
    "    group = df[df.sector==sector]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(mode='markers',\n",
    "                   x=group.PC1, y=group.PC2, \n",
    "                   z=group.PC3,\n",
    "                   text=group.project,\n",
    "                   marker=dict(\n",
    "                           size=10,\n",
    "                   ),\n",
    "                   name = sector,\n",
    "                   hovertemplate = '%{text}',\n",
    "                    )\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=f'World Bank ICR Reviews'\n",
    ")\n",
    "fig.update_traces(textposition='top center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eFe-dggQg8y"
   },
   "outputs": [],
   "source": [
    "# get sector embedding means\n",
    "sector_embs = {}\n",
    "\n",
    "for sector,g in df.groupby('sector', as_index=False):\n",
    "  if not g.empty:\n",
    "    sector_embs[sector] = g.mean_embedding.values\n",
    "\n",
    "mean_sector_embs = {}\n",
    "for sector in df.sector.unique():\n",
    "  mean_sector_embs[sector] = np.vstack(sector_embs[sector]).mean(axis=0)\n",
    "\n",
    "df['sector_mean'] = [mean_sector_embs[sector] for sector in df.sector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nG2Htrd3Rz69"
   },
   "outputs": [],
   "source": [
    "def dist_from_centroid(mean_embedding, sector_mean):\n",
    "  dist = np.linalg.norm(np.vstack(df.mean_embedding.values) - np.vstack(df.sector_mean.values), axis=1)\n",
    "  assert dist.shape == mean_embedding.shape\n",
    "  return dist\n",
    "\n",
    "df['dist_from_centroid'] = dist_from_centroid(df['mean_embedding'], df['sector_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woDy6YF5CgDv"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "sector_z = {}\n",
    "for sector in df.sector.unique():\n",
    "  z = np.abs(scipy.stats.zscore(df[df.sector==sector].dist_from_centroid))\n",
    "  sector_z[sector] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yB4_FARfGVu9"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_sector_zscore(grp):\n",
    "  print(grp['sector'].values[0])\n",
    "  sector = grp['sector'].values[0]\n",
    "  zscore = np.abs(scipy.stats.zscore(grp))  \n",
    "  return zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQrcQEY7FqXl"
   },
   "outputs": [],
   "source": [
    "# df['z_score'] = np.nan\n",
    "# for sector, group in df.groupby('sector'):\n",
    "#   df['z_score'] = df.dist_from_centroid.apply(lambda x: np.abs(scipy.stats.zscore(x)))\n",
    "df = df.groupby('sector').apply(lambda grp: grp.assign(zscore=np.abs(scipy.stats.zscore(grp.dist_from_centroid))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFCSsKu2I6e3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLs63h5xEJes",
    "outputId": "a438951a-7b97-42f5-a9e3-b93b6a59664b"
   },
   "outputs": [],
   "source": [
    "sector_mads = {sector: scipy.stats.median_absolute_deviation(df[df.sector == sector].dist_from_centroid) for sector in df.sector.unique()}\n",
    "print(\"{:>20} {:<10}\".format(\"Sector\", \"MAD\"))\n",
    "for sector in sector_mads:\n",
    "  print(f'{sector :>20} {sector_mads[sector]:<10.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "K-n3IiAR5VgH",
    "outputId": "38805679-93aa-4162-d73d-a13067d8a15d"
   },
   "outputs": [],
   "source": [
    "# df.dist_from_centroid.plot.density()\n",
    "import seaborn as sns\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "\n",
    "# box plot of the variable height\n",
    "ax = sns.boxplot(df.dist_from_centroid)\n",
    "\n",
    "fliers = boxplot_stats(df.dist_from_centroid).pop(0)['fliers']\n",
    "outliers = [y for stat in boxplot_stats(df['dist_from_centroid']) for y in fliers]\n",
    "# notation indicating an outlier\n",
    "# for y in outliers:\n",
    "  # ax.annotate('Outlier', xy=(y,0), xytext=(186,-0.05), fontsize=14,\n",
    "              # arrowprops=dict(arrowstyle='->', ec='grey', lw=2), bbox = dict(boxstyle=\"round\", fc=\"0.8\"))\n",
    "\n",
    "# for y in outliers:\n",
    "#     ax.plot(1, y, 'p')\n",
    "# ax.set_xlim(right=1.5)\n",
    "\n",
    "# xtick, label, and title\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlabel('distance from sector centroid', fontsize=14)\n",
    "plt.title('Distribution of distances', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "vtBC4mSUVT8z",
    "outputId": "795c13aa-abac-49f4-d455-f7efad1ed6ba"
   },
   "outputs": [],
   "source": [
    "sectors = ['Education', 'Health', 'Water/Sanit/Waste']\n",
    "# sectors = [x for x in df.sector.unique() if not x.startswith('(H)')]\n",
    "fig = plotly.subplots.make_subplots(rows=1, cols=1)\n",
    "\n",
    "for sector in sorted(sectors):\n",
    "    group = df[df.sector==sector]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(mode='markers',\n",
    "                   x=group.PC1, y=group.PC2, \n",
    "                   z=group.PC3,\n",
    "                   text=[f'Project: {row.project}<br>Sector Z-score: {row.dist_from_centroid:.3f}' for ind, row in group.iterrows()],\n",
    "                   marker=dict(\n",
    "                           size=10,\n",
    "                           color=group.zscore,\n",
    "                           cmin=0,\n",
    "                           cmax=df.zscore.max(),\n",
    "                           colorbar=dict(\n",
    "                             title=\"Z-Score\"\n",
    "                           ),\n",
    "                           colorscale='jet',\n",
    "                   ),\n",
    "                   name = sector,                   \n",
    "                   hovertemplate = \"%{text}\",\n",
    "                    )\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=f'World Bank ICR Review Embeddings<br>Color by sector Z-Score to detect outliers',\n",
    "    # coloraxis_colorbar_x=-0.15,\n",
    "    legend=dict(\n",
    "      yanchor=\"top\",\n",
    "      y=0.99,\n",
    "      xanchor=\"left\",\n",
    "      x=0.01\n",
    "    )\n",
    ")\n",
    "fig.update_traces(textposition='top center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "t99Ce0Wqb7gt",
    "outputId": "6dacb3e7-5278-4fcd-82a1-49eb8ad0b1a2"
   },
   "outputs": [],
   "source": [
    "sectors = ['Education', 'Health', 'Water/Sanit/Waste']\n",
    "# sectors = [x for x in df.sector.unique() if not x.startswith('(H)')]\n",
    "fig = plotly.subplots.make_subplots(rows=1, cols=1)\n",
    "\n",
    "for sector in df.sector.unique():\n",
    "    group = df[df.sector==sector]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(mode='markers',\n",
    "                   x=group.PC1, y=group.PC2, \n",
    "                   z=group.PC3,\n",
    "                   text=[f'Project: {row.project}<br>Sector Z-score: {row.dist_from_centroid:.3f}' for ind, row in group.iterrows()],\n",
    "                   marker=dict(\n",
    "                           size=10,\n",
    "                           color=group.zscore,\n",
    "                           cmin=0,\n",
    "                           cmax=df.zscore.max(),\n",
    "                           colorbar=dict(\n",
    "                             title=\"Z-Score\"\n",
    "                           ),\n",
    "                           colorscale='jet',\n",
    "                   ),\n",
    "                   name = sector,                   \n",
    "                   hovertemplate = \"%{text}\",\n",
    "                    )\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=f'World Bank ICR Review Embeddings<br>Color by sector Z-Score to detect outliers',\n",
    "    # coloraxis_colorbar_x=-0.15,\n",
    "    legend=dict(\n",
    "      yanchor=\"top\",\n",
    "      y=0.99,\n",
    "      xanchor=\"left\",\n",
    "      x=0.01\n",
    "    )\n",
    ")\n",
    "fig.update_traces(textposition='top center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "PPTj1rZBV-Ve",
    "outputId": "97e40f23-2a51-46e3-b7f3-3f892a7fc51e"
   },
   "outputs": [],
   "source": [
    "sectors = ['Education', 'Health', 'Water/Sanit/Waste']\n",
    "# sectors = [x for x in df.sector.unique() if not x.startswith('(H)')]\n",
    "fig = plotly.subplots.make_subplots(rows=1, cols=1)\n",
    "\n",
    "for sector in sorted(sectors):\n",
    "    group = df[df.sector==sector]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(mode='markers',\n",
    "                   x=group.PC1, y=group.PC2, \n",
    "                   z=group.PC3,\n",
    "                   text=[f'Project: {row.project}%\\nDistance from centroid: {row.dist_from_centroid.mean():.3f}' for ind, row in group.iterrows()],\n",
    "                   marker=dict(\n",
    "                           size=10,\n",
    "                   ),\n",
    "                   name = sector,                   \n",
    "                   hovertemplate = \"%{text}\",\n",
    "                    )\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=f'World Bank ICR Reviews'\n",
    ")\n",
    "fig.update_traces(textposition='top center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "450i3wPcdC4w"
   },
   "source": [
    "### All sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "v1FRNcZzcwMi",
    "outputId": "bc1f540f-4348-42a6-cedc-639438a84d07"
   },
   "outputs": [],
   "source": [
    "sectors = [x for x in df.sector.unique() if not x.startswith('(H)')]\n",
    "fig = plotly.subplots.make_subplots(rows=1, cols=1)\n",
    "\n",
    "for sector in sorted(sectors):\n",
    "    group = df[df.sector==sector]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(mode='markers',\n",
    "                   x=group.PC1, y=group.PC2, \n",
    "                   z=group.PC3,\n",
    "                   text=group.project,\n",
    "                   marker=dict(\n",
    "                           size=10,\n",
    "                   ),\n",
    "                   name = sector,\n",
    "                   hovertemplate = '%{text}',\n",
    "                    )\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=f'World Bank ICR Reviews'\n",
    ")\n",
    "fig.update_traces(textposition='top center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yTsqzTladTnl",
    "outputId": "a0058e19-477c-47e8-bdbf-a453c69155aa"
   },
   "outputs": [],
   "source": [
    "df.sector = df.sector.astype('category')\n",
    "y = df[\"sector\"].values\n",
    "X = np.vstack(df.mean_embedding.values)\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "model = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo')\n",
    "\n",
    "metrics = cross_validate(model, X, y, scoring=['precision_macro', 'recall_macro'], cv=cv, n_jobs=-1)\n",
    "\n",
    "print('Precision: %.3f (%.3f)' % (mean(metrics[\"test_precision_macro\"]), std(metrics[\"test_precision_macro\"])))\n",
    "print('Recall: %.3f (%.3f)' % (mean(metrics[\"test_recall_macro\"]), -std(metrics[\"test_recall_macro\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEqn8KlGgSY4",
    "outputId": "eab77346-cbb8-4c0d-db9f-5e2688594785"
   },
   "outputs": [],
   "source": [
    "# ## for bag-of-words\n",
    "# from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
    "\n",
    "# dtf = df.copy()\n",
    "\n",
    "# ## split dataset\n",
    "# dtf_train, dtf_test = model_selection.train_test_split(dtf, test_size=0.3, random_state=1)\n",
    "\n",
    "# X_train = np.vstack(dtf_train.mean_embedding.values)\n",
    "# X_test = np.vstack(dtf_test.mean_embedding.values)\n",
    "\n",
    "# ## get target\n",
    "# y_train = dtf_train[\"sector\"].values\n",
    "# y_test = dtf_test[\"sector\"].values\n",
    "\n",
    "# #Create a svm Classifier\n",
    "# clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "# #Train the model using the training sets\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# #Predict the response for test dataset\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # Model Accuracy: how often is the classifier correct?\n",
    "# print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# # Model Precision: what percentage of positive tuples are labeled as such?\n",
    "# print(\"Precision:\",metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# # Model Recall: what percentage of positive tuples are labelled as such?\n",
    "# print(\"Recall:\",metrics.recall_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CRZjsURiU3h"
   },
   "outputs": [],
   "source": [
    "# ## Accuracy, Precision, Recall\n",
    "# accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "# auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "#                             multi_class=\"ovr\")\n",
    "# print(\"Accuracy:\",  round(accuracy,2))\n",
    "# print(\"Auc:\", round(auc,2))\n",
    "# print(\"Detail:\")\n",
    "# print(metrics.classification_report(y_test, predicted))\n",
    "    \n",
    "# ## Plot confusion matrix\n",
    "# cm = metrics.confusion_matrix(y_test, predicted)\n",
    "# fig, ax = plt.subplots()\n",
    "# sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "#             cbar=False)\n",
    "# ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "#        yticklabels=classes, title=\"Confusion matrix\")\n",
    "# plt.yticks(rotation=0)\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "# ## Plot roc\n",
    "# for i in range(len(classes)):\n",
    "#     fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "#                            predicted_prob[:,i])\n",
    "#     ax[0].plot(fpr, tpr, lw=3, \n",
    "#               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "#                               metrics.auc(fpr, tpr))\n",
    "#                )\n",
    "# ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "# ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "#           xlabel='False Positive Rate', \n",
    "#           ylabel=\"True Positive Rate (Recall)\", \n",
    "#           title=\"Receiver operating characteristic\")\n",
    "# ax[0].legend(loc=\"lower right\")\n",
    "# ax[0].grid(True)\n",
    "    \n",
    "# ## Plot precision-recall curve\n",
    "# for i in range(len(classes)):\n",
    "#     precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "#                  y_test_array[:,i], predicted_prob[:,i])\n",
    "#     ax[1].plot(recall, precision, lw=3, \n",
    "#                label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "#                                   metrics.auc(recall, precision))\n",
    "#               )\n",
    "# ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "#           ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "# ax[1].legend(loc=\"best\")\n",
    "# ax[1].grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8iv4LMqkv-e",
    "outputId": "6950b503-3759-4377-e2f4-6d1bfb5bdb4e"
   },
   "outputs": [],
   "source": [
    "dtf = pd.DataFrame(report_embs).groupby(['file'], as_index=False).agg({'sentence': ' '.join})\n",
    "\n",
    "dtf['sector'] = dtf.file.apply(lambda x: FILE2SECTOR[x])\n",
    "drop_sectors = [s for s in dtf.sector.unique() if s.startswith('(H)')]\n",
    "dtf.drop(df[dtf.sector.isin(drop_sectors)].index, inplace=True)\n",
    "dtf.sector = dtf.sector.astype('category')\n",
    "\n",
    "dtf.rename(columns={'sentence': 'text','sector':'y'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlR4gMv3rSp0"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import feature_selection, metrics\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    '''\n",
    "    Preprocess a string.\n",
    "    :parameter\n",
    "        :param text: string - name of column containing text\n",
    "        :param lst_stopwords: list - list of stopwords to remove\n",
    "        :param flg_stemm: bool - whether stemming is to be applied\n",
    "        :param flg_lemm: bool - whether lemmitisation is to be applied\n",
    "    :return\n",
    "        cleaned text\n",
    "    '''\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "    \n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "dtf[\"text_clean\"] = dtf[\"text\"].apply(lambda x: \n",
    "          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, \n",
    "          lst_stopwords=lst_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6pzi8s8oeo7-",
    "outputId": "f50f9844-2a7b-4789-bd0c-7d85a3de264a"
   },
   "outputs": [],
   "source": [
    "## split dataset\n",
    "dtf_train, dtf_test = model_selection.train_test_split(dtf, test_size=0.2, stratify=dtf.y)\n",
    "## get target\n",
    "y_train = dtf_train[\"y\"].values\n",
    "y_test = dtf_test[\"y\"].values\n",
    "\n",
    "## Count (classic BoW)\n",
    "# vectorizer = feature_extraction.text.CountVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "## Tf-Idf (advanced variant of BoW)\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "corpus = dtf_train[\"text_clean\"]\n",
    "vectorizer.fit(corpus)\n",
    "X_train = vectorizer.transform(corpus)\n",
    "dic_vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "y = dtf_train[\"y\"]\n",
    "X_names = vectorizer.get_feature_names()\n",
    "p_value_limit = 0.95\n",
    "dtf_features = pd.DataFrame()\n",
    "for cat in np.unique(y):\n",
    "    chi2, p = feature_selection.chi2(X_train, y==cat)\n",
    "    dtf_features = dtf_features.append(pd.DataFrame(\n",
    "                   {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "    dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n",
    "                    ascending=[True,False])\n",
    "    dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "X_names = dtf_features[\"feature\"].unique().tolist()\n",
    "\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=X_names)\n",
    "vectorizer.fit(corpus)\n",
    "X_train = vectorizer.transform(corpus)\n",
    "dic_vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "classifier = naive_bayes.MultinomialNB()\n",
    "\n",
    "## pipeline\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train, y_train)\n",
    "## test\n",
    "X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_test)\n",
    "predicted_prob = model.predict_proba(X_test)\n",
    "\n",
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    \n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "    \n",
    "## Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "fig, ax = plt.subplots(figsize=(18,7))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "       yticklabels=classes, title=\"Confusion matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "dDTk2Z4XtLwm",
    "outputId": "4529b4e7-75e1-4143-bbb1-d6d07cde88aa"
   },
   "outputs": [],
   "source": [
    "!pip install -q lime\n",
    "from lime import lime_text\n",
    "\n",
    "## select observation\n",
    "i = 0\n",
    "txt_instance = dtf_test[\"text\"].iloc[i]\n",
    "## check true value and predicted value\n",
    "print(\"True:\", y_test[i], \"--> Pred:\", predicted[i], \"| Prob:\", round(np.max(predicted_prob[i]),2))\n",
    "## show explanation\n",
    "explainer = lime_text.LimeTextExplainer(class_names=\n",
    "             np.unique(y_train))\n",
    "explained = explainer.explain_instance(txt_instance, \n",
    "             model.predict_proba, num_features=3)\n",
    "explained.show_in_notebook(text=txt_instance, predict_proba=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVSgCw88vVhn"
   },
   "source": [
    "## Word2Vec Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "VT1U8ry0tvTC",
    "outputId": "d9f13836-e18e-4540-d9aa-1b2dbb45a9f4"
   },
   "outputs": [],
   "source": [
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "# nlp = gensim_api.load(\"word2vec-google-news-300\") # going to train own model instead\n",
    "\n",
    "corpus = dtf_train[\"text_clean\"]\n",
    "\n",
    "## create list of lists of unigrams\n",
    "lst_corpus = []\n",
    "for string in corpus:\n",
    "   lst_words = string.split()\n",
    "   lst_grams = [\" \".join(lst_words[i:i+1]) \n",
    "               for i in range(0, len(lst_words), 1)]\n",
    "   lst_corpus.append(lst_grams)\n",
    "\n",
    "## detect bigrams and trigrams\n",
    "bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, \n",
    "                 delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
    "trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], \n",
    "            delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
    "\n",
    "## fit w2v\n",
    "nlp = gensim.models.word2vec.Word2Vec(lst_corpus, size=300,   \n",
    "            window=8, min_count=1, sg=1, iter=30)\n",
    "\n",
    "word = \"health\"\n",
    "fig = plt.figure()\n",
    "## word embedding\n",
    "tot_words = [word] + [tupla[0] for tupla in \n",
    "                 nlp.most_similar(word, topn=20)]\n",
    "X = nlp[tot_words]\n",
    "## pca to reduce dimensionality from 300 to 3\n",
    "pca = manifold.TSNE(perplexity=40, n_components=3, init='pca')\n",
    "X = pca.fit_transform(X)\n",
    "## create dtf\n",
    "dtf_ = pd.DataFrame(X, index=tot_words, columns=[\"x\",\"y\",\"z\"])\n",
    "dtf_[\"input\"] = 0\n",
    "dtf_[\"input\"].iloc[0:1] = 1\n",
    "## plot 3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(dtf_[dtf_[\"input\"]==0]['x'], \n",
    "           dtf_[dtf_[\"input\"]==0]['y'], \n",
    "           dtf_[dtf_[\"input\"]==0]['z'], c=\"black\")\n",
    "ax.scatter(dtf_[dtf_[\"input\"]==1]['x'], \n",
    "           dtf_[dtf_[\"input\"]==1]['y'], \n",
    "           dtf_[dtf_[\"input\"]==1]['z'], c=\"red\")\n",
    "ax.set(xlabel=None, ylabel=None, zlabel=None, xticklabels=[], \n",
    "       yticklabels=[], zticklabels=[])\n",
    "for label, row in dtf_[[\"x\",\"y\",\"z\"]].iterrows():\n",
    "    x, y, z = row\n",
    "    ax.text(x, y, z, s=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 919
    },
    "id": "mpw9EU6Yup4p",
    "outputId": "618380e0-e663-4fa4-8261-d7fb25f02213"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "## tokenize text\n",
    "tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', \n",
    "                     oov_token=\"NaN\", \n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(lst_corpus)\n",
    "dic_vocabulary = tokenizer.word_index\n",
    "## create sequence\n",
    "lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)\n",
    "## padding sequence\n",
    "X_train = kprocessing.sequence.pad_sequences(lst_text2seq, \n",
    "                    maxlen=15, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "sns.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)\n",
    "plt.show()\n",
    "\n",
    "i = 0\n",
    "\n",
    "## list of text: [\"I like this\", ...]\n",
    "len_txt = len(dtf_train[\"text_clean\"].iloc[i].split())\n",
    "print(\"from: \", dtf_train[\"text_clean\"].iloc[i], \"| len:\", len_txt)\n",
    "\n",
    "## sequence of token ids: [[1, 2, 3], ...]\n",
    "len_tokens = len(X_train[i])\n",
    "print(\"to: \", X_train[i], \"| len:\", len(X_train[i]))\n",
    "\n",
    "## vocabulary: {\"I\":1, \"like\":2, \"this\":3, ...}\n",
    "print(\"check: \", dtf_train[\"text_clean\"].iloc[i].split()[0], \n",
    "      \" -- idx in vocabulary -->\", \n",
    "      dic_vocabulary[dtf_train[\"text_clean\"].iloc[i].split()[0]])\n",
    "\n",
    "print(\"vocabulary: \", dict(list(dic_vocabulary.items())[0:5]), \"... (padding element, 0)\")\n",
    "\n",
    "corpus = dtf_test[\"text_clean\"]\n",
    "\n",
    "## create list of n-grams\n",
    "lst_corpus = []\n",
    "for string in corpus:\n",
    "    lst_words = string.split()\n",
    "    lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, \n",
    "                 len(lst_words), 1)]\n",
    "    lst_corpus.append(lst_grams)\n",
    "    \n",
    "## detect common bigrams and trigrams using the fitted detectors\n",
    "lst_corpus = list(bigrams_detector[lst_corpus])\n",
    "lst_corpus = list(trigrams_detector[lst_corpus])\n",
    "## text to sequence with the fitted tokenizer\n",
    "lst_text2seq = tokenizer.texts_to_sequences(lst_corpus)\n",
    "\n",
    "## padding sequence\n",
    "X_test = kprocessing.sequence.pad_sequences(lst_text2seq, maxlen=15,\n",
    "             padding=\"post\", truncating=\"post\")\n",
    "\n",
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
    "for word,idx in dic_vocabulary.items():\n",
    "    ## update the row with vector\n",
    "    try:\n",
    "        embeddings[idx] =  nlp[word]\n",
    "    ## if word not in model then skip and the row stays all 0s\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "## code attention layer\n",
    "def attention_layer(inputs, neurons):\n",
    "    x = layers.Permute((2,1))(inputs)\n",
    "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "## input\n",
    "x_in = layers.Input(shape=(15,))\n",
    "## embedding\n",
    "x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                     output_dim=embeddings.shape[1], \n",
    "                     weights=[embeddings],\n",
    "                     input_length=15, trainable=False)(x_in)\n",
    "## apply attention\n",
    "x = attention_layer(x, neurons=15)\n",
    "## 2 layers of bidirectional lstm\n",
    "x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2, \n",
    "                         return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2))(x)\n",
    "## final dense layers\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "y_out = layers.Dense(3, activation='softmax')(x)\n",
    "## compile\n",
    "model = models.Model(x_in, y_out)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "8Xhk6Nk1u-_a",
    "outputId": "4e519904-a104-47c5-ed54-db6b9ab0c8ed"
   },
   "outputs": [],
   "source": [
    "## encode y\n",
    "dic_y_mapping = {n:label for n,label in \n",
    "                 enumerate(np.unique(y_train))}\n",
    "inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "y_train = np.array([inverse_dic[y] for y in y_train])\n",
    "## train\n",
    "training = model.fit(x=X_train, y=y_train, batch_size=256, \n",
    "                     epochs=10, shuffle=True, verbose=0, \n",
    "                     validation_split=0.3)\n",
    "## plot loss and accuracy\n",
    "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "ax[0].set(title=\"Training\")\n",
    "ax11 = ax[0].twinx()\n",
    "ax[0].plot(training.history['loss'], color='black')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "    ax11.plot(training.history[metric], label=metric)\n",
    "ax11.set_ylabel(\"Score\", color='steelblue')\n",
    "ax11.legend()\n",
    "ax[1].set(title=\"Validation\")\n",
    "ax22 = ax[1].twinx()\n",
    "ax[1].plot(training.history['val_loss'], color='black')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "     ax22.plot(training.history['val_'+metric], label=metric)\n",
    "ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shCKsAL_vRwH"
   },
   "outputs": [],
   "source": [
    "## test\n",
    "predicted_prob = model.predict(X_test)\n",
    "predicted = [dic_y_mapping[np.argmax(pred)] for pred in \n",
    "             predicted_prob]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6rhh76evDZR"
   },
   "source": [
    "## Compare DLI and ICR Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmDo02EjHdaP"
   },
   "outputs": [],
   "source": [
    "# report_mean_embeddings = []\n",
    "for report in report_embs:\n",
    "    file = report['file']\n",
    "    report_mean_embedding = np.vstack(report['embedding']).mean(0)\n",
    "    report_mean_embeddings.append({'file': file, 'embedding': report_mean_embedding})\n",
    "\n",
    "df = pd.DataFrame(report_mean_embeddings)\n",
    "df['sector'] = df.file.apply(lambda x: FILE2SECTOR[x])\n",
    "\n",
    "all_embeddings = np.stack(df.embedding.values)\n",
    "df = pd.concat([df, pd.DataFrame(project(all_embeddings, dims=3))], axis=1)\n",
    "df['project'] = df.file.apply(lambda x: FILE2ID[x])\n",
    "\n",
    "drop_sectors = [s for s in df.sector.unique() if s.startswith('(H)')]\n",
    "df.drop(df[df.sector.isin(drop_sectors)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcdmKfm_m5Mr"
   },
   "outputs": [],
   "source": [
    "dli_embeddings_reduced = pickle.load(open('dli_embeddings_reduced.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_Fe7tHKoKiy"
   },
   "outputs": [],
   "source": [
    "dli_embs = dli_embeddings_reduced['embedded_dlis']\n",
    "project_ids = dli_embeddings_reduced['project_ids']\n",
    "dli_df = pd.DataFrame({'dli_embs': [x for x in dli_embs]}, index = project_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6fPckW1uUF9"
   },
   "outputs": [],
   "source": [
    "icr_df = df[['project','embedding']].set_index('project')\n",
    "icr_embs = np.vstack(icr_df.embedding.values)\n",
    "stacked_embs = np.vstack((dli_embs, icr_embs))\n",
    "stacked_project_ids = dli_df.index.tolist() + icr_df.index.tolist()\n",
    "corpus = ['DLI'] * len(dli_embs) + ['ICR'] * len(icr_embs)\n",
    "# project into same PC space\n",
    "projected_embs = **project(stacked_embs, dims=3)\n",
    "df = pd.DataFrame({'project': stacked_project_ids, **projected_embs, 'corpus': corpus})\n",
    "df['sector'] = df.project.apply(lambda x: ID2SECTOR[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sunus_gBJ_YF"
   },
   "outputs": [],
   "source": [
    "# dli_embeddings_reduced = pickle.load(open('dli_embeddings_reduced.pkl','rb'))\n",
    "\n",
    "# project_ids = dli_embeddings_reduced['project_ids']\n",
    "# dlis = dli_embeddings_reduced['embedded_dlis']\n",
    "\n",
    "# PCs = project(dlis, dims=3)\n",
    "\n",
    "# dli_df = pd.DataFrame({'project': project_ids, **PCs})\n",
    "# dli_df['sector'] = dli_df.project.apply(lambda x: ID2SECTOR[x])\n",
    "\n",
    "# drop_sectors = [s for s in dli_df.sector.unique() if s.startswith('(H)')]\n",
    "# dli_df.drop(dli_df[dli_df.sector.isin(drop_sectors)].index, inplace=True)\n",
    "\n",
    "# df.set_index('project', inplace=True)\n",
    "# dli_df.set_index('project', inplace=True)\n",
    "\n",
    "# df_ = df.join(dli_df[['PC1','PC2','PC3']], lsuffix='_icr', rsuffix='_dli')\n",
    "# df_.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "OR1CFlvd3WsR",
    "outputId": "d2f6ffb0-78d0-4e9d-d3e3-8d18c58c6146"
   },
   "outputs": [],
   "source": [
    "# Plot hist of 2D embeddings\n",
    "X = icr_embs\n",
    "pca = PCA(n_components=2)\n",
    "projections = pca.fit_transform(X)\n",
    "_ = plt.hist2d(projections[:,0], projections[:,1])\n",
    "\n",
    "plt.figure()\n",
    "X = dli_embs\n",
    "pca = PCA(n_components=2)\n",
    "projections = pca.fit_transform(X)\n",
    "_ = plt.hist2d(projections[:,0], projections[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "on5XQlOFwemQ",
    "outputId": "b9276430-bd5d-4f07-f742-c46ffcd0c5cc"
   },
   "outputs": [],
   "source": [
    "for corpus, group in df.groupby('corpus'):\n",
    "  plt.figure()\n",
    "  group[['PC1','PC2','PC3']].plot.hist(bins=20, alpha=0.5, title=corpus + ' embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "yvSepBzbR3I5",
    "outputId": "acbacc3c-a5d4-4d9a-d6dd-42159051ba1a"
   },
   "outputs": [],
   "source": [
    "sector_focus = ['Education', 'Health' ,'Water/Sanit/Waste', ]\n",
    "fig = plotly.subplots.make_subplots(rows=1, cols=1)\n",
    "\n",
    "for (sector, corpus), group in df.groupby(['sector', 'corpus']):    \n",
    "    if sector not in sector_focus:\n",
    "        continue\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(mode='markers',\n",
    "                   x=group.PC1, y=group.PC2,\n",
    "                    #  z=sector_df.PC3_icr,\n",
    "                   text=group.project,\n",
    "                   marker=dict(\n",
    "                       symbol='diamond' if corpus is 'DLI' else 'circle',\n",
    "#                            opacity=0.5,\n",
    "    #                          color=2,\n",
    "                           size=10,\n",
    "    #                         colorscale='Viridis',\n",
    "    #                         line_width=1\n",
    "                   ),\n",
    "    #                    customdata = np.dstack((sector_df.sector.values, sector_df.report_id.values)),\n",
    "                   name = sector + '_' + corpus,\n",
    "                   hovertemplate = '%{text}',\n",
    "    #                      <br>Report: %{customdata[1]}',\n",
    "#                          fill=\"toself\",\n",
    "    #                visible='legendonly'\n",
    "                    )\n",
    "        )\n",
    "    \n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    # legend_title=\"Project Sectors\",\n",
    "    title_text=f'World Bank Project DLI vs ICR Embeddings'\n",
    ")\n",
    "fig.update_traces(textposition='top center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5J-GqjiS_O_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "PzKlZR0XCqYH",
    "cubXESvsCHrA"
   ],
   "name": "Report Embs",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
